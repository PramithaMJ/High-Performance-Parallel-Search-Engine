// This is a patch file to fix indentation and structure issues in crawler.c

/* In the crawl_website function, replace from approximately line 1100 to the end of the function with this properly structured code: */

            if (res == CURLE_OK && chunk.size > 100) {  // Ensure we have some content
                if (chunk.memory == NULL) {
                    fprintf(stderr, "  Downloaded content is NULL for %s\n", current_url);
                    failed_downloads++;
                } else {
                    // Save the content to a file, but check if we already downloaded it
                    // to avoid processing the same content twice
                    char* normalized_current = normalize_url(current_url);
                    int already_processed = 0;
                    
                    // Verify this URL hasn't already been processed in this session
                    for (int i = 0; i < visited_count; i++) {
                        if (strcmp(visited_urls[i], normalized_current) == 0) {
                            if (i < pages_crawled) {  // Only count as processed if it was successfully processed
                                already_processed = 1;
                                break;
                            }
                        }
                    }
                    
                    if (already_processed) {
                        printf("  Already processed this URL, skipping download\n");
                        // Still count as success
                        pages_crawled++;
                    } else {
                        // Download and process the URL
                        char* filename = download_url(current_url);
                        if (filename) {
                            printf("  Downloaded to %s (%zu bytes)\n", filename, chunk.size);
                            pages_crawled++;
                            failed_downloads = 0;  // Reset consecutive failure counter
                            
                            // If we have not reached max depth, extract links and add to queue
                            if (current_depth < maxDepth) {
                                char* extracted_urls[MAX_URLS];
                                int url_count = 0;
                                
                                // Pass the current URL as base URL for relative link resolution
                                extract_links(chunk.memory, current_url, extracted_urls, &url_count, MAX_URLS);
                                printf("  Found %d links\n", url_count);
                                
                                // Add new URLs to the queue
                                int added_urls = 0;
                                for (int i = 0; i < url_count && rear != (front - 1 + MAX_URLS) % MAX_URLS && added_urls < 20; i++) {
                                    // Make sure URL is not NULL before proceeding
                                    if (!extracted_urls[i]) continue;
                                    
                                    // First check if URL is valid for crawling before doing anything else
                                    // This avoids unnecessary processing and potential memory issues
                                    if (!is_valid_crawl_url(extracted_urls[i], base_domain)) {
                                        free(extracted_urls[i]); 
                                        extracted_urls[i] = NULL;
                                        continue;
                                    }
                                    
                                    // Check if already visited, but make a copy to compare
                                    char* url_copy = strdup(extracted_urls[i]);
                                    if (!url_copy) {
                                        // Memory allocation failure, just free and continue
                                        free(extracted_urls[i]);
                                        extracted_urls[i] = NULL;
                                        continue;
                                    }
                                    
                                    if (has_visited(url_copy)) {
                                        // Already visited, don't add to queue
                                        free(url_copy);
                                        free(extracted_urls[i]); 
                                        extracted_urls[i] = NULL;
                                        continue;
                                    }
                                    
                                    // URL is valid and not visited, add to queue
                                    free(url_copy); // Free the copy used for comparison
                                    queue[rear] = extracted_urls[i];
                                    depth[rear] = current_depth + 1;
                                    rear = (rear + 1) % MAX_URLS;
                                    
                                    // Now mark it as visited
                                    mark_visited(extracted_urls[i]);
                                    printf("  Queued: %s\n", extracted_urls[i]);
                                    added_urls++;
                                    
                                    // Mark as processed to avoid double-free
                                    extracted_urls[i] = NULL;
                                }
                                
                                // Free any remaining URLs
                                for (int i = 0; i < url_count; i++) {
                                    if (extracted_urls[i] != NULL) {
                                        free(extracted_urls[i]);
                                        extracted_urls[i] = NULL;
                                    }
                                }
                            }
                        } else {
                            failed_downloads++;
                            printf("  Failed to save content from: %s\n", current_url);
                        }
                    }
                }
            } else {
                failed_downloads++;
                fprintf(stderr, "  Failed to download: %s - %s\n", current_url, 
                        (res != CURLE_OK) ? curl_easy_strerror(res) : "Content too small");
            }
            
            // Safely free the memory chunk
            if (chunk.memory) {
                free(chunk.memory);
                chunk.memory = NULL;
            }
        } else {
            failed_downloads++;
            fprintf(stderr, "  Failed to initialize curl for: %s\n", current_url);
        }
        
        // Add a small delay between requests to be nice to servers (200-500ms)
        usleep((rand() % 300 + 200) * 1000);
        
        free(current_url);
    }
    
    // Clean up any remaining URLs in the queue
    while (front != rear) {
        if (queue[front] != NULL) {
            free(queue[front]);
            queue[front] = NULL;
        }
        front = (front + 1) % MAX_URLS;
    }
    
    // Clean up curl global state
    curl_global_cleanup();
    
    printf("\nCrawling completed. Crawled %d pages.\n", pages_crawled);
    return pages_crawled;
}

// Fixes for the Medium URL double-free issue:

1. Make sure URLs are properly normalized before adding to queue:
   - Use thread-local static buffers to prevent memory issues
   - Add special handling for Medium URLs with @username paths

2. During processing of extracted URLs, ensure:
   - URLs are normalized consistently
   - Duplicates are detected and freed properly
   - Medium URLs have proper waiting time between requests

3. Improved URL validation with Medium-specific checks:
   - Exclude problematic paths (signin, signout, etc.)
   - Add proper handling for Medium profile and article URLs
   - Add extra rate limiting for Medium requests
